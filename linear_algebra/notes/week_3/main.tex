%        File: main.tex
%     Created: Tue Sep 06 11:00 AM 2022 M
% Last Change: Tue Sep 06 11:00 AM 2022 M
%
\documentclass[a4paper]{article}
\input{preamble.tex}

\begin{document}

\part*{1.2: Standard Vector Spaces and Their Bases}

\subsection*{Definition:}

Given a vector space $V$, a basis of $v$ is a (finite) set of specific vectors, $\left\{ \vec{v_{1}}, \vec{v_{2}}, \dots, \vec{v_{n}} \right\}$

that are:

i: A system of generators

ii: Linearly independent


\subsection*{Theorem (Linear Independence)}

Given a vector space $V$ and a set of vectors $\left\{ \vec{v_{1}}, \vec{v_{2}}, \dots, \vec{v_{n}} \right\}$, the following statements are equivalent:

i: Our vectors are linear independent

ii: $\exists \vec{v} \in \left\{ \vec{v_1}, \vec{v_2}, \vec{v_n} \right\}$ that can be written as a linear decomp of the remaining ones.

iii: The equation $\alpha_1 \vec{v_1} + \alpha_2 \vec{v_2} + \dots + \alpha_3 \vec{v_3 = \vec{0}}$ has only the trivial solution $\alpha_1 = \alpha_2 = \dots = \alpha_n = 0$

\subsection*{Examples}
\subsubsection*{Example 1}

Consider the vectors
\[
\begin{Bmatrix}
	(1,0)^T, (1,1)^T, (2,6)^T
\end{Bmatrix}
\; \text{in} \; \bb{R}
\]

These are linearly dependent because we can write $\vec{v_3} = (2,6)^T$ as a linear combination of $(1,0)^t$ and $(1,1)^T$, namely:

\[
	\begin{pmatrix}
		2 \\
		6
	\end{pmatrix}
	=
	-4 \cdot \begin{pmatrix}
		1 \\
		0
	\end{pmatrix}
	+
	6 \cdot	\begin{pmatrix}
		1 \\
		1
	\end{pmatrix}
\]

\newpage
\section*{Standard Vector Spaces $\bb{R}^n, \bb{P}_{n}, M_{n \times m}$}
\subsection*{Euclidean Vector Spaces $\bb{R}^n$}

\begin{table}[h]
\begin{tabular}{lll}
Dimension & Standard Basis                          & Other Basis                                                                  \\
$\R^1$    & $e_1 = 1$                               & Any nonzero number                                                           \\
$\R^2$    & $\{ \vec{e}_1, \vec{e}_2 \}$            & \begin{tabular}[c]{@{}l@{}}Any 2 linearly\\ independent vectors\end{tabular} \\
$\R^3$    & $\{ \vec{e}_1, \vec{e}_2, \vec{e}_3 \}$ & \begin{tabular}[c]{@{}l@{}}Any 3 linearly\\ independent vectors\end{tabular} \\
$\R^n$	  & $\{ \vec{e}_1, \vec{e}_2, \dots, \vec{e}_n \}$ & \begin{tabular}[c]{@{}l@{}}Any $n$ linearly\\ independent vectors\end{tabular}
\end{tabular}
\end{table}

\textbf{NOTE:} To computationally check $\bb{R}^{3+}$ for linear independence that will be covered in Chapter 2 of the textbook. 

\subsection*{Polynomial Sets $\bb{P}_{n}$}

This is with polynomial addition and scalar multiplication

\begin{table}[h]
\begin{tabular}{lll}
Dimension & Standard Basis    & Other Basis                                                                                   \\
$\P^1$    & $\{1, t \}$       & \begin{tabular}[c]{@{}l@{}}Any 2 linearly \\ independent vectors\\ (polynomials)\end{tabular} \\
$\P^2$    & $\{ 1, t, t^2 \}$ & \begin{tabular}[c]{@{}l@{}}Any 3 linearly \\ independent vectors\\ (polynomials)\end{tabular}
\end{tabular}
\end{table}

\textbf{$\bb{P}_{2}$ (Quadratic)}

Standard Basis:

$\left\{ 1, t, t^2 \right\}$

\[
	7 + 10t - 2t^2 = 7 \vec{v_{1}} + 10 \vec{v_{2}} - 2 \vec{v_{3}}
\]

Thus, the vector $\vec{v}$ has coordinates $\left\{ 7, 10, -2 \right\}$ with regards to the standard basis.

\section*{$M_{n \times n}$: Matrix vector spaces and their basis}

\[
	M_{2 \times 2} = 
	\begin{Bmatrix}
		\begin{pmatrix}
			a & b \\
			cd & d
		\end{pmatrix} 
		| \; a,b,c,d \in \bb{R}
	\end{Bmatrix}
\]

\[
A = 
\begin{pmatrix}
	a & b \\
	cd & d
\end{pmatrix}
=
a? + b? + c? + d?
\]

\subsection*{Standard basis notation:}

$E_{ij}$ specifies the location in the matrix for the location of the ones where $i$ is the row and $j$ is the column. All other entries must be 0.

\[
	E_{11} = 
	\begin{pmatrix}
		1 & 0 \\
		0 & 0
	\end{pmatrix}
\]

\subsubsection*{Symmetric Matrices}

Symmetric means we must have a square matrix.

$2 \times 2$

\[
A = 
\begin{pmatrix}
	a & b \\
	c & d
\end{pmatrix}
\]

\[
A^T = 
\begin{pmatrix}
	a & c \\
	b & d
\end{pmatrix}
\]

So, this is symmetric if $b = d$.

The standard basis then are:

\[
	\begin{pmatrix}
		1 & 0 \\
		0 & 0
	\end{pmatrix},
	\begin{pmatrix}
		0 & 1 \\
		1 & 0
	\end{pmatrix},
	\begin{pmatrix}
		0 & 0 \\
		0 & 1
	\end{pmatrix}
\]

\subsection*{Symmetric $n \times n$}

\[
	\begin{pmatrix}
		a_{11} & a_{12}  & \dots & a_{1n} \\
		a_{21} & a_{22} & \dots & a_{2n} \\
		\dots & \dots & \dots & \dots \\
	\end{pmatrix}
\]

So, to figure out the basis for this guy we can take advantage of the fact that $a_{12} = a_{2a}$ to say the following:

1st row needs $n$ parameters. But, one of those parameters covers ONE of the points in the next row. So\ldots

2nd row needs $n - 1$ parameters. 

3rd row needs $n - 2$ parameters and so on. The final row then needs one extra parameter. 

So a basis has $n + (n - 1) + (n - 2) + \dots + 1$ vectors, or $\frac{n(n + 1)}{2}$.

\pagebreak
\part*{1.3: Linear Transformations}

Question, consider the basis of $\bb{R}^3, \bb{p_{3}}, M_{2 \times 2}$. How are they the same?

\section*{Part 1: What is the equivalent of functions for vector spaces?} 

\fbox{\begin{minipage}{30em}
	A linear transformation is a mapping
	
	\[T: V \to W\]
	
	Where $V$ and $W$ are vector spaces such that

	\[\forall \vec{u}, \vec{v} \in V, T(\vec{u} + \vec{v}) = T(\vec{u}) + T(\vec{v})\]

and

	\[\forall \vec{u} \in V, \forall \alpha \in \bb{R}, T(\alpha \vec{u}) = \alpha T(\vec{u})\]
\end{minipage}
}

\subsection*{Example 1: The derivative operation}

\[\frac{d}{dt}: \bb{P}_{3} \to \bb{P}_{2}\]

\[\frac{d}{dt}(p) = p'\]

\[\frac{d}{dt}(7 - t + t^2 + 5t^3) = -1 + 2t + 15t^2\]

Then 

\[\frac{d}{dt}(p_1 + p_2) = \frac{d}{dt}(p_1) + \frac{d}{dt}(p_2)\]

\[\frac{d}{dt}(cp) = c \frac{d}{dt}(p)\]

\subsection*{Example 2: Rotating the plane by $\frac{\pi}{2}$ degrees.}

$\vec{v} = (x, y)^T$

$T(\vec{v} = (-y, x)^T)$

The map

\[T: \bb{R}^2 \to \bb{R}^2\]

\[T \psmall{x \\ y} = \psmall{-y \\ x} \]

is a linear transformation.

\subsubsection{Justification:}

i: $\forall \vec{u}, \vec{v} \in V$, vector addition works as expected

Let $\vec{u} = \psmall{u_1 \\ u_2}$ and $\vec{v} = \psmall{v_1 \\ v_2}$ be two vectors.

Then $T(\vec{u}) = \psmall{-u_2 \\ u_1}$, and $T(\vec{v}) = \psmall{-v_2 \\ v_1}$. 

So 

\[T(\vec{u}) + T(\vec{v}) = 
\begin{pmatrix}
	-u_2 - v_2 \\
	u_1 + v_1
\end{pmatrix}
\]

ii: Scalar multiplication

\[\forall \vec{u} \in V, \forall \alpha \in \bb{R}, T(\alpha \vec{u}) = \alpha T(\vec{u})\]

I'm behind on notes here, just know this works as expected.

\subsection*{Conclusion:}

The big takeaway with transformations is to isolate the basis vectors and see what happens to them. From this you can scale those basis vectors to see where you end up. Everything follows the basis vectors.

\end{document}


